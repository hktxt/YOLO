{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### codes reference https://github.com/ydixon/yolo_v3/blob/master/yolo_detect.ipynb\n",
    "### a nice bolg for understanding. https://blog.csdn.net/leviopku/article/details/82660381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic network building blocks - conv_bn_relu, res_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://blog.csdn.net/leviopku/article/details/82660381, resn\n",
    "class conv_bn_relu(nn.Module):\n",
    "    def __init__(self, nin, nout, kernel, stride=1, pad=\"SAME\", padding=0, bn=True, activation=\"leakyRelu\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bn = bn\n",
    "        self.activation = activation\n",
    "        \n",
    "        if pad == 'SAME':\n",
    "            padding = (kernel-1)//2\n",
    "            \n",
    "        self.conv = nn.Conv2d(nin, nout, kernel, stride, padding, bias=not bn)\n",
    "        if bn == True:\n",
    "            self.bn = nn.BatchNorm2d(nout)\n",
    "        if activation == \"leakyRelu\":\n",
    "            self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "    \n",
    "class res_layer(nn.Module):\n",
    "    def __init__(self, nin):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_bn_relu(nin, nin//2, kernel=1)  #64->32, 1\n",
    "        self.conv2 = conv_bn_relu(nin//2, nin, kernel=3)  #32->64, 3 see figure of darknet\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.conv2(self.conv1(x)) # just '+', the dim will be the same, not concat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map2cfgDict - used to creating mapping that follows the cfg file from prjreddit's repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map2cfgDict(mlist):\n",
    "    idx = 0 \n",
    "    mdict = OrderedDict()\n",
    "    for i,m in enumerate(mlist):\n",
    "        if isinstance(m, res_layer):\n",
    "            mdict[idx] = None\n",
    "            mdict[idx+1] = None\n",
    "            idx += 2\n",
    "        mdict[idx] = i\n",
    "        idx += 1\n",
    "    \n",
    "    return mdict        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UpsampleGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UpsampleGroup: conv + upsample + concat \n",
    "# see https://blog.csdn.net/leviopku/article/details/82660381, DBL + 上采样 + concat\n",
    "class UpsampleGroup(nn.Module):\n",
    "    def __init__(self, nin):\n",
    "        super().__init__()\n",
    "        self.conv = conv_bn_relu(nin, nin//2, kernel=1)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        \n",
    "    def forward(self, route_head, route_tail):\n",
    "        out = self.up(self.conv(route_head))\n",
    "        return torch.cat((out, route_tail), 1) # concat, size: nin/2 + nin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Darknet53 - Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_res_stack(nin, num_block):\n",
    "    return nn.ModuleList([conv_bn_relu(nin, nin*2, 3, stride=2)] + [res_layer(nin*2) for n in range(num_block)])\n",
    "\n",
    "class Darknet(nn.Module):\n",
    "    def __init__(self, blkList, nout=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlist = nn.ModuleList()\n",
    "        self.mlist += [conv_bn_relu(3, nout, 3)]\n",
    "        for i,nb in enumerate(blkList):\n",
    "            self.mlist += make_res_stack(nout*(2**i), nb)\n",
    "            \n",
    "        self.map2yolocfg = map2cfgDict(self.mlist)\n",
    "        self.cachedOutDict = dict()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i,m in enumerate(self.mlist):\n",
    "            x = m(x)\n",
    "            if i in self.cachedOutDict:\n",
    "                self.cachedOutDict[i] = x\n",
    "        return x\n",
    "    \n",
    "    def addCachedOut(self, idx, mode=\"yolocfg\"):\n",
    "        if mode == \"yolocfg\":\n",
    "            idxs = self.map2yolocfg[idx]\n",
    "        self.cachedOutDict[idxs] = None\n",
    "        \n",
    "    def getCachedOut(self, idx, mode=\"yolocfg\"):\n",
    "        if mode == \"yolocfg\":\n",
    "            idxs = self.map2yolocfg[idx]\n",
    "        return self.cachedOutDict[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Darknet([1,2,8,8,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.addCachedOut(61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 13, 13])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.pdf'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from visualization import *\n",
    "\n",
    "inputs = torch.randn(1,3,416,416)\n",
    "net = Darknet([1,2,8,8,4])\n",
    "y = net(Variable(inputs))\n",
    "print(y.shape)\n",
    "\n",
    "g = make_dot(y, net.state_dict());\n",
    "g.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PreDetectionConvGroup - conv layers before the yolo detection layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreDetectionConvGroup(nn.Module):\n",
    "    def __init__(self, nin, nout, num_conv=3, numClass=80):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mlist = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_conv): # 2*3 = 6,see figure of conv set on https://blog.csdn.net/qq_37541097/article/details/81214953 \n",
    "            self.mlist += [conv_bn_relu(nin, nout, kernel=1)]\n",
    "            self.mlist += [conv_bn_relu(nout, nout*2, kernel=3)]\n",
    "            if i == 0:\n",
    "                nin = nout*2\n",
    "                \n",
    "        self.mlist += [nn.Conv2d(nin, (numClass+5)*3, 1)] # expand dim to (numClass+5)*3\n",
    "        self.map2yolocfg = map2cfgDict(self.mlist)\n",
    "        self.cachedOutDict = dict()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i,m in enumerate(self.mlist):\n",
    "            x = m(x)\n",
    "            if i in self.cachedOutDict:\n",
    "                self.cachedOutDict[i] = x\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    #mode - normal  -- direct index to mlist \n",
    "    #     - yolocfg -- index follow the sequences of the cfg file from https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg\n",
    "    def addCachedOut(self, idx, mode=\"yolocfg\"):\n",
    "        if mode == \"yolocfg\":\n",
    "            idx = self.getIdxFromYoloIdx(idx)\n",
    "        elif idx < 0:\n",
    "            idx = len(self.mlist) - idx\n",
    "        \n",
    "        self.cachedOutDict[idx] = None\n",
    "        \n",
    "    def getCachedOut(self, idx, mode=\"yolocfg\"):\n",
    "        if mode == \"yolocfg\":\n",
    "            idx = self.getIdxFromYoloIdx(idx)\n",
    "        elif idx < 0:\n",
    "            idx = len(self.mlist) - idx\n",
    "        return self.cachedOutDict[idx]\n",
    "    \n",
    "    def getIdxFromYoloIdx(self,idx):\n",
    "        if idx < 0:\n",
    "            return len(self.map2yolocfg) + idx\n",
    "        else:\n",
    "            return self.map2yolocfg[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conv layer before do pred 1, and it is also applied on another two pred.\n",
    "# see https://blog.csdn.net/qq_37541097/article/details/81214953\n",
    "# convolution set(5) + conv = 6\n",
    "mlist = nn.ModuleList()\n",
    "numClass = 80\n",
    "nin = 1024\n",
    "nout = 512\n",
    "for i in range(3):\n",
    "    mlist += [conv_bn_relu(nin, nout, kernel=1)]\n",
    "    mlist += [conv_bn_relu(nout, nout*2, kernel=3)]\n",
    "    if i == 0:\n",
    "        nin = nout*2\n",
    "        \n",
    "mlist += [nn.Conv2d(nin, (numClass+5)*3, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): conv_bn_relu(\n",
       "    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  )\n",
       "  (1): conv_bn_relu(\n",
       "    (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  )\n",
       "  (2): conv_bn_relu(\n",
       "    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  )\n",
       "  (3): conv_bn_relu(\n",
       "    (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  )\n",
       "  (4): conv_bn_relu(\n",
       "    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  )\n",
       "  (5): conv_bn_relu(\n",
       "    (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): LeakyReLU(negative_slope=0.1, inplace)\n",
       "  )\n",
       "  (6): Conv2d(1024, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreDetectionConvGroup(\n",
       "  (mlist): ModuleList(\n",
       "    (0): conv_bn_relu(\n",
       "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (1): conv_bn_relu(\n",
       "      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (2): conv_bn_relu(\n",
       "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (3): conv_bn_relu(\n",
       "      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (4): conv_bn_relu(\n",
       "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (5): conv_bn_relu(\n",
       "      (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): LeakyReLU(negative_slope=0.1, inplace)\n",
       "    )\n",
       "    (6): Conv2d(1024, 255, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_det1 = PreDetectionConvGroup(1024, 512, numClass=80);pre_det1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(pre_det1.addCachedOut(-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yolo Detection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLayer(nn.Module):\n",
    "    def __init__(self, anchors, img_dim, nClass):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.anchors = anchors # [(116, 90), (156, 198), (373, 326)]\n",
    "        self.img_dim = img_dim\n",
    "        self.nClass = nClass\n",
    "        self.bbox_attrib = nClass + 5\n",
    "        \n",
    "    def forward(self, x, img_dim):\n",
    "        # x: bs * nA(5 + nClass) * h * w\n",
    "        nB = x.shape[0]    # batch_size\n",
    "        nA = len(self.anchors) # 3\n",
    "        nH, nW = x.shape[2], x.shape[3]\n",
    "        stride = img_dim[1] / nH # 416/13=32\n",
    "        anchors = torch.FloatTensor(self.anchors) / stride\n",
    "        \"\"\"\n",
    "        tensor([[ 3.6250,  2.8125],\n",
    "                [ 4.8750,  6.1875],\n",
    "                [11.6562, 10.1875]])\n",
    "        \"\"\"\n",
    "        \n",
    "        ##Reshape predictions from [B x [A * (5 + numClass)] x H x W] to [B x A x H x W x (5 + numClass)]\n",
    "        # like:[1, 3, 85, 416, 416]->[1, 3, 416, 416, 85], see https://www.zhihu.com/question/60321866 for details.\n",
    "        preds = x.view(nB, nA, self.bbox_attrib, nH, nW).permute(0, 1, 3, 4, 2).contiguous()\n",
    "        \n",
    "        # tx, ty, tw, wh\n",
    "        preds_xy = preds[..., :2]            # = [:,:,:,:,:2], shape:torch.Size([1, 3, 416, 416, 2])\n",
    "        preds_wh = preds[..., 2:4]           # [x, y, w, h, 3-84]\n",
    "        preds_conf = preds[..., 4].sigmoid() # [x, y, w, h, conf, 4-84]\n",
    "        preds_cls = preds[..., 5:].sigmoid() # [x, y, w, h, conf, cls(80)]\n",
    "        \n",
    "        # Calculate cx, cy, anchor mesh\n",
    "        mesh_x = torch.arange(nW).repeat(nH,1).unsqueeze(2)              # H * W * 1\n",
    "        mesh_y = torch.arange(nH).repeat(nW,1).t().unsqueeze(2)          # H * W * 1\n",
    "        mesh_xy = torch.cat((mesh_x,mesh_y), 2)                          # H * W * 2\n",
    "        mesh_anchors = anchors.view(1, nA, 1, 1, 2).repeat(1, 1, nH, nW, 1) # 1 * nA * 1 * 1 * 2 -> 1 * nA * H * W * 2\n",
    "        \n",
    "        # pred_boxes holds bx,by,bw,bh\n",
    "        pred_boxes = torch.FloatTensor(preds[..., :4].shape)  # [1, 3, 416, 416, 4]\n",
    "        #pred_boxes[..., :2] = preds_xy.detach().cpu().sigmoid() + mesh_xy\n",
    "        pred_boxes[..., :2] = preds_xy.detach().cpu().sigmoid() + mesh_xy.float() # sig(tx) + cx, detach(): http://www.bnikolic.co.uk/blog/pytorch-detach.html\n",
    "        pred_boxes[..., 2:4] = preds_wh.detach().cpu().exp() * mesh_anchors  # exp(tw) * anchor\n",
    "        \n",
    "        # Return predictions if not training # # [1, 3, 416, 416, 85]\n",
    "        \"\"\"\n",
    "        out = torch.cat((pred_boxes.cuda() * stride, \n",
    "                         preds_conf.cuda().unsqueeze(4),\n",
    "                         preds_cls.cuda() ), 4)\n",
    "        \"\"\"\n",
    "        out = torch.cat((pred_boxes * stride, \n",
    "                         preds_conf.unsqueeze(4),\n",
    "                         preds_cls), 4)\n",
    "        \n",
    "        # Reshape predictions from [B x A x H x W x (5 + numClass)] to [B x [A x H x W] x (5 + numClass)]\n",
    "        # such that predictions at different strides could be concatenated on the same dimension\n",
    "        out = out.permute(0, 2, 3, 1, 4).contiguous().view(nB, nA*nH*nW, self.bbox_attrib)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 255, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.randn([1, 1024, 13, 13])\n",
    "o = pre_det1(inp)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 507, 85])\n"
     ]
    }
   ],
   "source": [
    "yolo = YoloLayer(anchors[0], 416, 80)\n",
    "inp = torch.randn([1, 255, 13, 13]).float()\n",
    "p = yolo(inp, 416)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 13, 13])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_det1.getCachedOut(-3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 26, 26])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn([1, 3, 416, 416])\n",
    "feature = Darknet([1,2,8,8,4])\n",
    "feature.addCachedOut(61)\n",
    "op = feature(inp)\n",
    "feature.getCachedOut(61).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 26, 26])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_head1 = torch.randn([1, 512, 13, 13])\n",
    "r_tail1 = torch.randn([1, 512, 26, 26])\n",
    "up1 = UpsampleGroup(512)\n",
    "out = up1(r_head1, r_tail1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 255, 26, 26])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_det2 = PreDetectionConvGroup(768, 256, 80)\n",
    "inp = torch.randn([1, 768, 26, 26])\n",
    "out = pre_det2(inp)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2028, 85])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo2 = YoloLayer(anchors[1], 416, 80)\n",
    "inp = torch.randn([1, 255, 26, 26])\n",
    "det2 = yolo2(out, 416)\n",
    "det2.shape # 26*26*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_det2 = PreDetectionConvGroup(768, 256, numClass=80)\n",
    "pre_det2.addCachedOut(-3)\n",
    "inp = torch.randn([1, 768, 26, 26])\n",
    "o = pre_det2(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 26, 26])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_det2.getCachedOut(-3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 52, 52])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn([1, 3, 416, 416])\n",
    "feature = Darknet([1,2,8,8,4])\n",
    "feature.addCachedOut(36)\n",
    "op = feature(inp)\n",
    "feature.getCachedOut(36).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384, 52, 52])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_head2 = torch.randn([1, 256, 26, 26])\n",
    "r_tail2 = torch.randn([1, 256, 52, 52])\n",
    "up2 = UpsampleGroup(256)\n",
    "out = up2(r_head2, r_tail2)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 255, 52, 52])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_det3 = PreDetectionConvGroup(384, 128, numClass=80)\n",
    "inp = torch.randn([1, 384, 52, 52])\n",
    "out = pre_det3(inp)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8112, 85])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo3 = YoloLayer(anchors[2], 416, 80)\n",
    "inp = torch.randn([1, 255, 52, 52])\n",
    "det3 = yolo3(out, 416)\n",
    "det3.shape # 52*52*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entire network - putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloNet(nn.Module):\n",
    "    def __init__(self, img_dim=None, anchors=[10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326], numClass=80):\n",
    "        super().__init__()\n",
    "        \n",
    "        nin = 32\n",
    "        self.numClass = numClass\n",
    "        self.img_dim = img_dim\n",
    "        self.stat_keys = ['loss', 'loss_x', 'loss_y', 'loss_w', 'loss_h', 'loss_conf', 'loss_cls',\n",
    "                          'nCorrect', 'nGT', 'recall']\n",
    "        \n",
    "        anchors = [(anchors[i], anchors[i+1]) for i in range(0,len(anchors),2)]\n",
    "        anchors = [anchors[i:i+3] for i in range(0, len(anchors), 3)][::-1]\n",
    "        \"\"\"\n",
    "        [[(116, 90), (156, 198), (373, 326)],\n",
    "         [(30, 61), (62, 45), (59, 119)],\n",
    "         [(10, 13), (16, 30), (33, 23)]]\n",
    "        \"\"\"\n",
    "        self.feature = Darknet([1,2,8,8,4]) # darknet 53 \n",
    "        self.feature.addCachedOut(61)\n",
    "        self.feature.addCachedOut(36)\n",
    "        \n",
    "        self.pre_det1 = PreDetectionConvGroup(1024, 512, numClass=self.numClass) # 6 + 1 conv layer,see above \n",
    "        self.yolo1 = YoloLayer(anchors[0], img_dim, self.numClass) # [(116, 90), (156, 198), (373, 326)]\n",
    "        self.pre_det1.addCachedOut(-3) #Fetch output from 4th layer backward including yolo layer\n",
    "        \n",
    "        self.up1 = UpsampleGroup(512)\n",
    "        self.pre_det2 = PreDetectionConvGroup(768, 256, numClass=self.numClass)\n",
    "        self.yolo2 = YoloLayer(anchors[1], img_dim, self.numClass) # [(30, 61), (62, 45), (59, 119)]\n",
    "        self.pre_det2.addCachedOut(-3)\n",
    "        \n",
    "        self.up2 = UpsampleGroup(256)\n",
    "        self.pre_det3 = PreDetectionConvGroup(384, 128, numClass=self.numClass)\n",
    "        self.yolo3 = YoloLayer(anchors[2], img_dim, self.numClass) # [(10, 13), (16, 30), (33, 23)]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        img_dim = (x.shape[3], x.shape[2]) # w, h\n",
    "\n",
    "        # extract features\n",
    "        out = self.feature(x) # out = torch.Size([1, 1024, 13, 13])\n",
    "\n",
    "        # detection layer 1\n",
    "        out = self.pre_det1(out) # torch.Size([1, 255, 13, 13]) , 255=3*(80+5)\n",
    "        det1 = self.yolo1(out, img_dim) # torch.Size([1, 507, 85]), 507=13*13*3\n",
    "\n",
    "        # upsample 1\n",
    "        r_head1 = self.pre_det1.getCachedOut(-3) # torch.Size([1, 512, 13, 13])\n",
    "        r_tail1 = self.feature.getCachedOut(61)  # torch.Size([1, 512, 26, 26])\n",
    "        out = self.up1(r_head1, r_tail1) # torch.Size([1, 768, 26, 26]), 256 + 512, see UpsampleGroup\n",
    "\n",
    "        # detection layer 2\n",
    "        out = self.pre_det2(out) # torch.Size([1, 255, 26, 26])  , 255=3*(80+5), like pre_det1\n",
    "        det2 = self.yolo2(out, img_dim) # torch.Size([1, 2028, 85]), 2028 = 26*26*3\n",
    "\n",
    "        # upsample 2\n",
    "        r_head2 = self.pre_det2.getCachedOut(-3) # torch.Size([1, 256, 26, 26])\n",
    "        r_tail2 = self.feature.getCachedOut(36)  # torch.Size([1, 256, 52, 52])\n",
    "        out = self.up2(r_head2, r_tail2) # torch.Size([1, 384, 52, 52]), 256+128=384\n",
    "\n",
    "        # detection layer 3\n",
    "        out = self.pre_det3(out) # torch.Size([1, 255, 52, 52]) , 255=3*(80+5), like pre_det 1,2\n",
    "        det3 = self.yolo3(out, img_dim) # torch.Size([1, 8112, 85]), 8112=52*52*3\n",
    "\n",
    "        return det1, det2, det3  # torch.Size([1, 507, 85]), torch.Size([1, 2028, 85]), torch.Size([1, 8112, 85])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](yolo_v3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Max\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    }
   ],
   "source": [
    "inp = torch.rand([1,3,416,416])\n",
    "YNet = YoloNet()\n",
    "det1, det2, det3 = YNet(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 507, 85]),\n",
       " torch.Size([1, 2028, 85]),\n",
       " torch.Size([1, 8112, 85]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det1.shape, det2.shape, det3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10647, 85])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections = torch.cat((det1,det2,det3), 1);detections.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IOU and non-max supression(NMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_vectorized(bbox): # torch.Size([41, 7]), 41 depends on predictions, [b1_x, b1_y, b2_x, b2_y, obj_conf, class_score, class]\n",
    "    num_box = bbox.shape[0] # 41\n",
    "    \n",
    "    bbox_leftTop_x = bbox[:,0] # torch.Size([41])\n",
    "    bbox_leftTop_y = bbox[:,1] # torch.Size([41])\n",
    "    bbox_rightBottom_x = bbox[:,2] # torch.Size([41])\n",
    "    bbox_rightBottom_y = bbox[:,3] # torch.Size([41])\n",
    "    \n",
    "    inter_leftTop_x = torch.max(bbox_leftTop_x.unsqueeze(1).repeat(1,num_box), bbox_leftTop_x) # torch.Size([41, 41])\n",
    "    inter_leftTop_y = torch.max(bbox_leftTop_y.unsqueeze(1).repeat(1,num_box), bbox_leftTop_y)\n",
    "    inter_rightBottom_x = torch.min(bbox_rightBottom_x.unsqueeze(1).repeat(1,num_box), bbox_rightBottom_x)\n",
    "    inter_rightBottom_y = torch.min(bbox_rightBottom_y.unsqueeze(1).repeat(1,num_box), bbox_rightBottom_y)\n",
    "    \n",
    "    inter_area = torch.clamp(inter_rightBottom_x - inter_leftTop_x, min=0) * torch.clamp(inter_rightBottom_y - inter_leftTop_y, min=0) ## torch.Size([41, 41])\n",
    "    bbox_area = (bbox_rightBottom_x - bbox_leftTop_x) * (bbox_rightBottom_y - bbox_leftTop_y) # torch.Size([41])\n",
    "    union_area = bbox_area.expand(num_box, -1) + bbox_area.expand(num_box, -1).transpose(0,1) - inter_area # torch.Size([41, 41])\n",
    "    \n",
    "    iou = inter_area / union_area # torch.Size([41, 41])\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([41, 41])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through the bounding boxes and remove rows accordingly\n",
    "def reduce_row_by_column(inp):\n",
    "    i = 0\n",
    "    while i < inp.shape[0]: # 41\n",
    "        remove_row_idx = inp[i][1].item()\n",
    "        if inp[i][0] != remove_row_idx and i < inp.shape[0]:\n",
    "            keep_mask = (inp[:,0] != remove_row_idx).nonzero().squeeze()\n",
    "            inp = inp[keep_mask]\n",
    "        i += 1\n",
    "        \n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bbox is expected to be sorted by class score in descending order\n",
    "def nms(bbox, iou, nms_thres):\n",
    "    #Create a mapping that indicates which row has iou > threshold\n",
    "    remove_map = (iou > nms_thres).nonzero() # torch.Size([41, 2])\n",
    "    remove_map = reduce_row_by_column(remove_map)\n",
    "    \n",
    "    remove_idx = torch.unique(remove_map[:,0])\n",
    "    res_bbox = bbox[remove_idx]\n",
    "    \n",
    "    return res_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-processing - convert predictions from network to bounding boxes (calls IOU/NMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(detections, num_classes, obj_conf_thr=0.5, nms_thr=0.4):\n",
    "    \n",
    "    # Zero bounding box with objectioness confidence score less than threshold\n",
    "    obj_conf_filter = (detections[:,:,4] > obj_conf_thr).float().unsqueeze(2) # torch.Size([1, 10647, 1])\n",
    "    detections = detections * obj_conf_filter\n",
    "    \n",
    "    #Transform bounding box coordinates to two corners, from centorid to corner\n",
    "    box = detections.new(detections[:,:,:4].shape) # torch.Size([1, 10647, 4])\n",
    "    box[:,:,0] = detections[:,:,0] - detections[:,:,2]/2\n",
    "    box[:,:,1] = detections[:,:,1] - detections[:,:,3]/2\n",
    "    box[:,:,2] = box[:,:,0] + detections[:,:,2]\n",
    "    box[:,:,3] = box[:,:,1] + detections[:,:,3]\n",
    "    detections[:,:,:4] = box\n",
    "    \n",
    "    num_batches = detections.shape[0]\n",
    "    \n",
    "    results = list()\n",
    "    \n",
    "    for b in range(num_batches):\n",
    "        #batch_results = torch.Tensor().cuda()\n",
    "        batch_results = torch.Tensor()\n",
    "        img_det = detections[b] # torch.Size([10647, 85])\n",
    "        \n",
    "        max_class_score, max_class_idx = torch.max(img_det[:,5:5+num_classes], 1) # torch.Size([10647]), torch.Size([10647])\n",
    "        img_det = torch.cat((img_det[:,:5], # torch.Size([10647, 5])\n",
    "                           max_class_score.float().unsqueeze(1), # torch.Size([10647, 1])\n",
    "                           max_class_idx.float().unsqueeze(1)), # torch.Size([10647, 1])\n",
    "                           1)\n",
    "        #img det - [b1_x, b1_y, b2_x, b2_y, obj_conf, class_score, class], torch.Size([10647, 7])\n",
    "        \n",
    "        #Remove zeroed rows, < obj_conf_thr will be removed.\n",
    "        nonzero_idx = img_det[:,4].nonzero() # Return the indices of the elements that are non-zero. torch.Size([5105, 1])\n",
    "        img_det = img_det[nonzero_idx,:].view(-1,7) # torch.Size([5105, 7])\n",
    "        \n",
    "        if img_det.shape[0] == 0:\n",
    "            #results.append(batch_results.cpu())\n",
    "            results.append(batch_results)\n",
    "        else:\n",
    "            # get the classes\n",
    "            img_classes = torch.unique(img_det[:,-1]) # torch.Size([80])\n",
    "            for c in img_classes:\n",
    "                # Select rows with \"c\" class and sort by the class score\n",
    "                class_img_det = img_det[(img_det[:,-1] == c).nonzero().squeeze()]\n",
    "                # If there is only one detection, it will return a 1D tensor. Therefore, we perform a view to keep it in 2D\n",
    "                class_img_det = class_img_det.view(-1,7)\n",
    "                #Sort by objectness score\n",
    "                _, sort_idx = class_img_det[:,4].sort(descending=True)\n",
    "                class_img_det = class_img_det[sort_idx]\n",
    "                \n",
    "                iou = iou_vectorized(class_img_det) # torch.Size([41, 41])\n",
    "                #Alert: There's another loop operation in nms function\n",
    "                class_img_det = nms(class_img_det, iou, nms_thr)\n",
    "                batch_results = torch.cat((batch_results, class_img_det), 0)\n",
    "                \n",
    "            #results.append(batch_results.cpu())\n",
    "            results.append(batch_results)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10647, 85]),\n",
       " tensor([[331.3665, -10.9208, 349.6289,  ...,   0.5518,   0.5647,  60.0000],\n",
       "         [220.0789,  23.0935, 307.1667,  ...,   0.6094,   0.6346,  60.0000],\n",
       "         [293.7178, 232.7700, 331.5706,  ...,   0.5858,   0.5942,  60.0000],\n",
       "         ...,\n",
       "         [128.0964, 100.9296, 136.8837,  ...,   0.5062,   0.6574,  53.0000],\n",
       "         [190.9442,  78.4702, 200.4171,  ...,   0.5062,   0.6237,  53.0000],\n",
       "         [246.5906, 214.9729, 257.4446,  ...,   0.5052,   0.6380,  53.0000]],\n",
       "        grad_fn=<CatBackward>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdet = postprocessing(detections, 80, obj_conf_thr=0.5)\n",
    "detections.shape, pdet[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5513, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[331.3665, -10.9208, 349.6289,  ...,   0.5518,   0.5647,  60.0000],\n",
       "         [220.0789,  23.0935, 307.1667,  ...,   0.6094,   0.6346,  60.0000],\n",
       "         [293.7178, 232.7700, 331.5706,  ...,   0.5858,   0.5942,  60.0000],\n",
       "         ...,\n",
       "         [128.0964, 100.9296, 136.8837,  ...,   0.5062,   0.6574,  53.0000],\n",
       "         [190.9442,  78.4702, 200.4171,  ...,   0.5062,   0.6237,  53.0000],\n",
       "         [246.5906, 214.9729, 257.4446,  ...,   0.5052,   0.6380,  53.0000]],\n",
       "        grad_fn=<CatBackward>)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pdet[0].shape)\n",
    "pdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
