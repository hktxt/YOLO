{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the implementation of YOLOv3 loss function. It is differnet from v1. see https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before this, let's recap what have we done.  Dataloader reads image data and annotations. Darknet53, this net extracts features. Yolo_layers do the detections.\n",
    "#### The yolo layers return 3 scale outputs: (13\\*13 + 26\\*26 + 52\\*52)\\*3\\*(80+4+1). Here we using outputs and annotations to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_targets(model, targets, pred): #\n",
    "    # targets: torch.Size([2, 6]), from the dataloader, 2 means num of objs readed from batch iamges. index + 类别 + 坐标 = 6, see 04_DataLoader.ipynb\n",
    "    # pred:  pred: [[1,3,13,13,85],[1,3,26,26,85],[1,3,52,52,85]] from the net output\n",
    "    \n",
    "    # targets = [image, class, x, y, w, h]\n",
    "    \n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model = model.module\n",
    "    yolo_layers = get_yolo_layers(model) # [82, 94, 106] for yolov3\n",
    "\n",
    "    # anchors = closest_anchor(model, targets)  # [layer, anchor, i, j]\n",
    "    txy, twh, tcls, tconf, indices = [], [], [], [], []\n",
    "    for i, layer in enumerate(yolo_layers):\n",
    "        nG = model.module_list[layer][0].nG  # grid size 13,26,52\n",
    "        anchor_vec = model.module_list[layer][0].anchor_vec #size:[3,2],tensor([[ 3.62500,  2.81250],[ 4.87500,  6.18750],[11.65625, 10.18750]]) ? \n",
    "\n",
    "        # iou of targets-anchors\n",
    "        gwh = targets[:, 4:6] * nG # w,h * nG, tensor([[ 6.81248,  1.68392],[11.79978,  8.35538]]), 2 obj * w,h\n",
    "        iou = [wh_iou(x, gwh) for x in anchor_vec] # [3,2], 3 anchor_vec * 2 gwh\n",
    "        iou, a = torch.stack(iou, 0).max(0)  # best iou and anchor\n",
    "        # 来自dataloader的targets里是每个batch中的obj的坐标，将坐标转化为grid scale大小，计算与anchors的iou，找出最大的iou和对应的anchor\n",
    "\n",
    "        # reject below threshold ious (OPTIONAL)\n",
    "        reject = True\n",
    "        if reject:\n",
    "            j = iou > 0.01\n",
    "            t, a, gwh = targets[j], a[j], gwh[j]\n",
    "        else:\n",
    "            t = targets\n",
    "\n",
    "        # Indices\n",
    "        b, c = t[:, 0:2].long().t()  # target image, class， b:img index, c:class label\n",
    "        gxy = t[:, 2:4] * nG\n",
    "        gi, gj = gxy.long().t()  # grid_i, grid_j\n",
    "        indices.append((b, a, gj, gi))\n",
    "\n",
    "        # XY coordinates\n",
    "        txy.append(gxy - gxy.floor())\n",
    "\n",
    "        # Width and height\n",
    "        twh.append(torch.log(gwh / anchor_vec[a]))  # yolo method\n",
    "        # twh.append(torch.sqrt(gwh / anchor_vec[a]) / 2)  # power method\n",
    "\n",
    "        # Class\n",
    "        tcls.append(c)\n",
    "\n",
    "        # Conf\n",
    "        tci = torch.zeros_like(pred[i][..., 0])\n",
    "        tci[b, a, gj, gi] = 1  # conf\n",
    "        tconf.append(tci)\n",
    "\n",
    "    return txy, twh, tcls, tconf, indices\n",
    "    # txy: list,3, 0:2*2, 1:2*2, 2:1*2\n",
    "    # twh: list,3, 0:2*2, 1:2*2, 2:1*2\n",
    "    # tcls: list,3, torch.Size([2]), torch.Size([2]), torch.Size([2])\n",
    "    # tconf: list,3, torch.Size([b, 3, 13, 13]), torch.Size([b, 3, 26, 26]), torch.Size([b, 3, 52, 52])\n",
    "    # indices: list,3, tuple(4), tuple(4), tuple(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(p, targets):  # predictions: [[1,3,13,13,85],[1,3,26,26,85],[1,3,52,52,85]] from the net output; targets: txy, twh, tcls, tconf, indices\n",
    "    FT = torch.cuda.FloatTensor if p[0].is_cuda else torch.FloatTensor\n",
    "    loss, lxy, lwh, lcls, lconf = FT([0]), FT([0]), FT([0]), FT([0]), FT([0])\n",
    "    txy, twh, tcls, tconf, indices = targets \n",
    "    MSE = nn.MSELoss()\n",
    "    CE = nn.CrossEntropyLoss()\n",
    "    BCE = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Compute losses\n",
    "    # gp = [x.numel() for x in tconf]  # grid points\n",
    "    for i, pi0 in enumerate(p):  # layer i predictions, i\n",
    "        b, a, gj, gi = indices[i]  # image, anchor, gridx, gridy\n",
    "\n",
    "        # Compute losses\n",
    "        k = 1  # nT / bs\n",
    "        if len(b) > 0:\n",
    "            pi = pi0[b, a, gj, gi]  # predictions closest to anchors\n",
    "            lxy += k * MSE(torch.sigmoid(pi[..., 0:2]), txy[i])  # xy\n",
    "            lwh += k * MSE(pi[..., 2:4], twh[i])  # wh\n",
    "            lcls += (k / 4) * CE(pi[..., 5:], tcls[i])\n",
    "\n",
    "        # pos_weight = FT([gp[i] / min(gp) * 4.])\n",
    "        # BCE = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        lconf += (k * 64) * BCE(pi0[..., 4], tconf[i])\n",
    "    loss = lxy + lwh + lconf + lcls\n",
    "\n",
    "    # Add to dictionary\n",
    "    d = defaultdict(float)\n",
    "    losses = [loss.item(), lxy.item(), lwh.item(), lconf.item(), lcls.item()]\n",
    "    for name, x in zip(['total', 'xy', 'wh', 'conf', 'cls'], losses):\n",
    "        d[name] = x\n",
    "\n",
    "    return loss, d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
