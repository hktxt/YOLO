{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi box IOU calcuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode - x1y1x2y2, cxcywh\n",
    "def bbox_iou(b1, b2, mode=\"x1y1x2y2\"):\n",
    "    if mode == \"x1y1x2y2\":\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = b1[...,0], b1[...,1], b1[...,2], b1[...,3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = b2[...,0], b2[...,1], b2[...,2], b2[...,3]  \n",
    "    elif mode == \"cxcywh\":\n",
    "        b1_x1, b1_x2 = b1[..., 0] - b1[..., 2] / 2, b1[..., 0] + b1[..., 2] / 2\n",
    "        b1_y1, b1_y2 = b1[..., 1] - b1[..., 3] / 2, b1[..., 1] + b1[..., 3] / 2\n",
    "        b2_x1, b2_x2 = b2[..., 0] - b2[..., 2] / 2, b2[..., 0] + b2[..., 2] / 2\n",
    "        b2_y1, b2_y2 = b2[..., 1] - b2[..., 3] / 2, b2[..., 1] + b2[..., 3] / 2\n",
    "    \n",
    "    num_b1 = b1.shape[0]\n",
    "    num_b2 = b2.shape[0]\n",
    "    \n",
    "    inter_x1 = torch.max(b1_x1.unsqueeze(1).repeat(1, num_b2), b2_x1)\n",
    "    inter_y1 = torch.max(b1_y1.unsqueeze(1).repeat(1, num_b2), b2_y1)\n",
    "    inter_x2 = torch.min(b1_x2.unsqueeze(1).repeat(1, num_b2), b2_x2)\n",
    "    inter_y2 = torch.min(b1_y2.unsqueeze(1).repeat(1, num_b2), b2_y2)\n",
    "            \n",
    "    inter_area = torch.clamp(inter_x2 - inter_x1, min=0) * torch.clamp(inter_y2 - inter_y1, min=0)\n",
    "    b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n",
    "    b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n",
    "    union_area = b1_area.unsqueeze(1).repeat(1, num_b2) + b2_area.unsqueeze(0).repeat(num_b1, 1) - inter_area\n",
    "    \n",
    "    iou = inter_area / union_area\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yolo Loss layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLayer(nn.Module):\n",
    "    def __init__(self, anchors, img_dim, numClass):\n",
    "        super().__init__()\n",
    "        self.anchors = anchors\n",
    "        self.img_dim = img_dim\n",
    "                \n",
    "        self.numClass = numClass\n",
    "        self.bbox_attrib = 5 + numClass\n",
    "        \n",
    "        self.lambda_xy = 1\n",
    "        self.lambda_wh = 1\n",
    "        self.lambda_conf = 1 #1.0\n",
    "        self.lambda_cls = 1 #1.0\n",
    "        \n",
    "        self.obj_scale = 1 #5\n",
    "        self.noobj_scale = 1 #1\n",
    "        \n",
    "        self.ignore_thres = 0.5\n",
    "        \n",
    "        self.mseloss = nn.MSELoss(size_average=False) # https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss\n",
    "        self.bceloss = nn.BCELoss(size_average=False)\n",
    "        # https://pytorch.org/docs/stable/nn.html#bceloss\n",
    "        self.bceloss_average = nn.BCELoss(size_average=True) # Binary Cross Entropy between the target and the output\n",
    " \n",
    "    def forward(self, x, img_dim, target=None):\n",
    "        #x : bs x nA*(5 + num_classes) * h * w\n",
    "        nB = x.shape[0]\n",
    "        nA = len(self.anchors)\n",
    "        nH, nW = x.shape[2], x.shape[3]\n",
    "        stride = img_dim[1] / nH\n",
    "        anchors = torch.FloatTensor(self.anchors) / stride\n",
    "        \n",
    "        #Reshape predictions from [B x [A * (5 + numClass)] x H x W] to [B x A x H x W x (5 + numClass)]\n",
    "        preds = x.view(nB, nA, self.bbox_attrib, nH, nW).permute(0, 1, 3, 4, 2).contiguous()\n",
    "        \n",
    "        # tx, ty, tw, wh\n",
    "        preds_xy = preds[..., :2]\n",
    "        preds_wh = preds[..., 2:4]\n",
    "        preds_conf = preds[..., 4].sigmoid()\n",
    "        preds_cls = preds[..., 5:].sigmoid()\n",
    "        \n",
    "        # Calculate cx, cy, anchor mesh\n",
    "        mesh_x = torch.arange(nW).repeat(nH,1).unsqueeze(2)\n",
    "        mesh_y = torch.arange(nH).repeat(nW,1).t().unsqueeze(2)\n",
    "        mesh_xy = torch.cat((mesh_x,mesh_y), 2)\n",
    "        mesh_anchors = anchors.view(1, nA, 1, 1, 2).repeat(1, 1, nH, nW, 1)\n",
    "        \n",
    "        # pred_boxes holds bx,by,bw,bh\n",
    "        pred_boxes = torch.FloatTensor(preds[..., :4].shape)\n",
    "        pred_boxes[..., :2] = preds_xy.detach().cpu().sigmoid() + mesh_xy.float() # sig(tx) + cx\n",
    "        pred_boxes[..., 2:4] = preds_wh.detach().cpu().exp() * mesh_anchors  # exp(tw) * anchor\n",
    "        \n",
    "        if target is not None:\n",
    "            obj_mask, noobj_mask, tconf, tcls, tx, ty, tw, th, nCorrect, nGT = self.build_target_tensor(\n",
    "                                                                    pred_boxes, target.detach().cpu(),\n",
    "                                                                    anchors, (nH, nW), self.numClass,\n",
    "                                                                    self.ignore_thres)\n",
    "            \n",
    "            #recall = float(nCorrect / nGT) if nGT else 1\n",
    "            #assert(nGT == TP + FN)\n",
    "\n",
    "            # masks for loss calculations\n",
    "            #obj_mask, noobj_mask = obj_mask.cuda(), noobj_mask.cuda()\n",
    "            cls_mask = (obj_mask == 1)\n",
    "            #tconf, tcls = tconf.cuda(), tcls.cuda()\n",
    "            #tx, ty, tw, th = tx.cuda(), ty.cuda(), tw.cuda(), th.cuda()\n",
    "\n",
    "            loss_x = self.lambda_xy * self.mseloss(preds_xy[..., 0] * obj_mask, tx * obj_mask) / nB\n",
    "            loss_y = self.lambda_xy * self.mseloss(preds_xy[..., 1] * obj_mask, ty * obj_mask) / nB\n",
    "            loss_w = self.lambda_wh * self.mseloss(preds_wh[..., 0] * obj_mask, tw * obj_mask) / nB\n",
    "            loss_h = self.lambda_wh * self.mseloss(preds_wh[..., 1] * obj_mask, th * obj_mask) / nB\n",
    "\n",
    "            loss_conf = self.lambda_conf * \\\n",
    "                        ( self.obj_scale * self.bceloss(preds_conf * obj_mask, obj_mask) + \\\n",
    "                          self.noobj_scale * self.bceloss(preds_conf * noobj_mask, noobj_mask * 0) ) / nB\n",
    "            loss_cls = self.lambda_cls * self.bceloss(preds_cls[cls_mask], tcls[cls_mask]) / nB\n",
    "            loss =  loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls \n",
    "                \n",
    "            return loss, loss.item(), loss_x.item(), loss_y.item(), loss_w.item(), loss_h.item(), \\\n",
    "                   loss_conf.item(), loss_cls.item(), \\\n",
    "                   nCorrect, nGT\n",
    "           \n",
    "        # Return predictions if not training \n",
    "        out = torch.cat((pred_boxes * stride, \n",
    "                         preds_conf.unsqueeze(4),\n",
    "                         preds_cls ), 4)\n",
    "        \n",
    "        # Reshape predictions from [B x A x H x W x (5 + numClass)] to [B x [A x H x W] x (5 + numClass)]\n",
    "        # such that predictions at different strides could be concatenated on the same dimension\n",
    "        out = out.permute(0, 2, 3, 1, 4).contiguous().view(nB, nA*nH*nW, self.bbox_attrib)\n",
    "        return out\n",
    "\n",
    "    def build_target_tensor(self, pred_boxes, target, anchors, inp_dim, numClass, ignore_thres):\n",
    "        nB = target.shape[0] # batch\n",
    "        nA = len(anchors) # 3\n",
    "        nH, nW = inp_dim[0], inp_dim[1]\n",
    "        nCorrect = 0\n",
    "        nGT = 0\n",
    "        target = target.float()\n",
    "\n",
    "        obj_mask = torch.zeros(nB, nA, nH, nW, requires_grad=False)\n",
    "        noobj_mask = torch.ones(nB, nA, nH, nW, requires_grad=False)\n",
    "        tconf= torch.zeros(nB, nA, nH, nW, requires_grad=False)\n",
    "        tcls= torch.zeros(nB, nA, nH, nW, numClass, requires_grad=False)\n",
    "        tx = torch.zeros(nB, nA, nH, nW, requires_grad=False)\n",
    "        ty = torch.zeros(nB, nA, nH, nW, requires_grad=False)\n",
    "        tw = torch.zeros(nB, nA, nH, nW, requires_grad=False)\n",
    "        th = torch.zeros(nB, nA, nH, nW, requires_grad=False)\n",
    "\n",
    "        for b in range(nB): # batches\n",
    "            for t in range(target.shape[1]): # targets\n",
    "                if target[b, t].sum() == 0:\n",
    "                    break;\n",
    "                nGT += 1\n",
    "\n",
    "                gx = target[b, t, 1] * nW\n",
    "                gy = target[b, t, 2] * nH\n",
    "                gw = target[b, t, 3] * nW\n",
    "                gh = target[b, t, 4] * nH\n",
    "                gi = int(gx)\n",
    "                gj = int(gy)\n",
    "\n",
    "                # pred_boxes - [A x H x W x 4]  \n",
    "                # Do not train for objectness(noobj) if anchor iou > threshold.\n",
    "                tmp_gt_boxes = torch.FloatTensor([gx, gy, gw, gh]).unsqueeze(0)\n",
    "                tmp_pred_boxes = pred_boxes[b].view(-1, 4)\n",
    "                tmp_ious, _ = torch.max(bbox_iou(tmp_pred_boxes, tmp_gt_boxes, mode=\"cxcywh\"), 1)\n",
    "                ignore_idx = (tmp_ious > ignore_thres).view(nA, nH, nW) # get rid of box that iou > thres with GT.\n",
    "                noobj_mask[b][ignore_idx] = 0\n",
    "\n",
    "                \n",
    "                #find best fit anchor for each ground truth box\n",
    "                tmp_gt_boxes = torch.FloatTensor([[0, 0, gw, gh]])\n",
    "                tmp_anchor_boxes = torch.cat((torch.zeros(nA, 2), anchors), 1)\n",
    "                tmp_ious = bbox_iou(tmp_anchor_boxes, tmp_gt_boxes, mode=\"cxcywh\")\n",
    "                best_anchor = torch.argmax(tmp_ious, 0).item()\n",
    "                \n",
    "                #find iou for best fit anchor prediction box against the ground truth box\n",
    "                tmp_gt_box = torch.FloatTensor([gx, gy, gw, gh]).unsqueeze(0)\n",
    "                tmp_pred_box = pred_boxes[b, best_anchor, gj, gi].view(-1, 4)\n",
    "                tmp_iou = bbox_iou(tmp_gt_box, tmp_pred_box, mode=\"cxcywh\")\n",
    "\n",
    "                if tmp_iou > 0.5:\n",
    "                    nCorrect += 1\n",
    "\n",
    "                obj_mask[b, best_anchor, gj, gi] = 1\n",
    "                #noobj_mask[b, best_anchor, gj, gi] = 0\n",
    "                tconf[b, best_anchor, gj, gi] = 1\n",
    "                tcls[b, best_anchor, gj, gi, int(target[b, t, 0])] = 1\n",
    "                sig_x = gx - gi\n",
    "                sig_y = gy - gj\n",
    "                tx[b, best_anchor, gj, gi] = torch.log(sig_x/(1-sig_x) + 1e-16)\n",
    "                ty[b, best_anchor, gj, gi] = torch.log(sig_y/(1-sig_y) + 1e-16)\n",
    "                tw[b, best_anchor, gj, gi] = torch.log(gw / anchors[best_anchor, 0] + 1e-16)\n",
    "                th[b, best_anchor, gj, gi] = torch.log(gh / anchors[best_anchor, 1] + 1e-16)\n",
    "\n",
    "        return obj_mask, noobj_mask, tconf, tcls, tx, ty, tw, th, nCorrect, nGT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify YoloNet (from yolo_detect.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import darknet\n",
    "#from darknet import Darknet, PreDetectionConvGroup, UpsampleGroup, WeightManager\n",
    "\n",
    "class YoloNet(nn.Module):\n",
    "    def __init__(self, img_dim=None, anchors = [10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326], numClass=80):\n",
    "        super().__init__()\n",
    "        nin = 32\n",
    "        self.numClass = numClass\n",
    "        self.img_dim = img_dim\n",
    "        self.stat_keys = ['loss', 'loss_x', 'loss_y', 'loss_w', 'loss_h', 'loss_conf', 'loss_cls',\n",
    "                          'nCorrect', 'nGT', 'recall']\n",
    "        \n",
    "        anchors = [(anchors[i], anchors[i+1]) for i in range(0,len(anchors),2)]\n",
    "        anchors = [anchors[i:i+3] for i in range(0, len(anchors), 3)][::-1]\n",
    "                \n",
    "        self.feature = Darknet([1,2,8,8,4])\n",
    "        self.feature.addCachedOut(61)\n",
    "        self.feature.addCachedOut(36)\n",
    "        \n",
    "        self.pre_det1 = PreDetectionConvGroup(1024, 512, numClass=self.numClass)\n",
    "        self.yolo1 = YoloLayer(anchors[0], img_dim, self.numClass)\n",
    "        self.pre_det1.addCachedOut(-3) #Fetch output from 4th layer backward including yolo layer\n",
    "        \n",
    "        self.up1 = UpsampleGroup(512)\n",
    "        self.pre_det2 = PreDetectionConvGroup(768, 256, numClass=self.numClass)\n",
    "        self.yolo2 = YoloLayer(anchors[1], img_dim, self.numClass)\n",
    "        self.pre_det2.addCachedOut(-3)\n",
    "        \n",
    "        self.up2 = UpsampleGroup(256)\n",
    "        self.pre_det3 = PreDetectionConvGroup(384, 128, numClass=self.numClass)\n",
    "        self.yolo3 = YoloLayer(anchors[2], img_dim, self.numClass)\n",
    "        \n",
    "   \n",
    "    def forward(self, x, target=None):\n",
    "        img_dim = (x.shape[3], x.shape[2])\n",
    "        #Extract features\n",
    "        out = self.feature(x)\n",
    "                \n",
    "        #Detection layer 1\n",
    "        out = self.pre_det1(out)\n",
    "        det1 = self.yolo1(out, img_dim, target)\n",
    "        \n",
    "        #Upsample 1\n",
    "        r_head1 = self.pre_det1.getCachedOut(-3)\n",
    "        r_tail1 = self.feature.getCachedOut(61)\n",
    "        out = self.up1(r_head1,r_tail1)\n",
    "                \n",
    "        #Detection layer 2\n",
    "        out = self.pre_det2(out)\n",
    "        det2 = self.yolo2(out, img_dim, target)\n",
    "        \n",
    "        #Upsample 2\n",
    "        r_head2 = self.pre_det2.getCachedOut(-3)\n",
    "        r_tail2 = self.feature.getCachedOut(36)\n",
    "        out = self.up2(r_head2,r_tail2)\n",
    "                \n",
    "        #Detection layer 3\n",
    "        out = self.pre_det3(out)\n",
    "        det3 = self.yolo3(out, img_dim, target)\n",
    "        \n",
    "        if target is not None:\n",
    "            loss, *out = [sum(det) for det in zip(det1, det2, det3)]\n",
    "            self.stats = dict(zip(self.stat_keys, out))\n",
    "            self.stats['recall'] = self.stats['nCorrect'] / self.stats['nGT'] if self.stats['nGT'] else 0\n",
    "            return loss\n",
    "        else:\n",
    "            return det1, det2, det3\n",
    "    \n",
    "    # Format : pytorch / darknet\n",
    "    def saveWeight(self, weights_path, format='pytorch'):\n",
    "        if format == 'pytorch':\n",
    "            torch.save(self.state_dict(), weights_path)\n",
    "        elif format == 'darknet':\n",
    "            raise NotImplementedError\n",
    "    \n",
    "    def loadWeight(self, weights_path, format='pytorch'):\n",
    "        if format == 'pytorch':\n",
    "            weights = torch.load(weights_path, map_location=lambda storage, loc: storage)\n",
    "            self.load_state_dict(weights)\n",
    "        elif format == 'darknet':\n",
    "            wm = WeightManager(self)\n",
    "            wm.loadWeight(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand([1,3,416,416])\n",
    "YNet = YoloNet()\n",
    "det1, det2, det3 = YNet(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 507, 85]),\n",
       " torch.Size([1, 2028, 85]),\n",
       " torch.Size([1, 8112, 85]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det1.shape,det2.shape,det3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.rand([1, 2, 5]) # Batch * obj * 5(1+4)\n",
    "inp = torch.rand([1,3,416,416])\n",
    "YNet = YoloNet()\n",
    "loss = YNet(inp, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8092.1797, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
